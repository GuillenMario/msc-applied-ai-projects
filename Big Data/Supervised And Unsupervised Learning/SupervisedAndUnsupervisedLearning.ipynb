{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfMQuxm2g1Iz"
   },
   "source": [
    "## **Master in Applied Artificial Intelligence**\n",
    "### **Course: Artificial Intelligence and Automatic Learning**\n",
    "### Prof. Iván Olmos\n",
    "#### Tecnológico de Monterrey\n",
    "\n",
    "### **Supervised and Unsupervised Learning**\n",
    "##### **Name and enrollment number:**\n",
    "* Mario Guillen de la Torre - A01796701"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCNJTWznIgcT"
   },
   "source": [
    "## **I. Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ooOMAGKmId-"
   },
   "source": [
    "Machine learning algorithms are mainly divided into two categories: supervised and unsupervised algorithms. Each one expects a different data structure and is used to solve problems of different natures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1.1 Supervised Algorithms**\n",
    "Supervised algorithms expect a dependent or “target” variable whose relationship with the independent variables can be analyzed to find patterns and generate a predictive model. The data type of the dependent variable, whether categorical or continuous, will determine the type of problem to address: classification or regression.\n",
    "\n",
    "In classification problems, the goal is to predict the discrete category to which a new record belongs. Classic algorithms include:\n",
    "\n",
    "- Naive Bayes classifier\n",
    "- Vector support machines (SVMs)\n",
    "- Logistics regression\n",
    "- Decision trees (and its derived algorithms as decision forests)\n",
    "\n",
    "The PySpark MLlib library provides implementations of all these algorithms.\n",
    "\n",
    "In regression problems, the goal is to predict a continuous value based on the independent variables. PySpark includes implementations of the following algorithms (among others):\n",
    "\n",
    "- Decision tree returns.\n",
    "- GBT Rots (gradient boosted trees)\n",
    "- FM Rots (Machines Factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1.2 Unsupervised Algorithms**\n",
    "Unsupervised algorithms do not have a “target” variable. Instead, they aim to find patterns, structures, or groupings that are intrinsically present in the analyzed data. These patterns can be used for decision-making and dimensionality reduction.\n",
    "\n",
    "A common use case is the search for frequent patterns in the data, meaning identifying recurring relationships among variables. PySpark implements two different algorithms for this purpose:\n",
    "\n",
    "- FPGrowth\n",
    "- Prefixspan\n",
    "\n",
    "Another approach is clustering analysis, where the goal is to assign each record to a group and determine which group a new record belongs to. Common clustering algorithms implemented by PySpark include:\n",
    "\n",
    "- LDA\n",
    "- Gaussianmixture\n",
    "- Kmeans\n",
    "- Bisectingkmeans\n",
    "- PoweriteratiClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **II. Data Selection**\n",
    "As mentioned in previous notebooks, partitioning rules were established based on three characterization variables, selected for their relevance in identifying behavioral patterns:\n",
    "\n",
    "- Payment_Group: Groups payment methods into Credit Card, Cash, Mobile, and Other. _This variable is fundamental, as there is empirical evidence that passengers who pay with a card tend to leave tips more frequently than those who pay with cash._\n",
    "\n",
    "- Pickup_zone_group: Corresponds to the starting area of the trip. The most representative community areas were grouped as: 76, 8, 32, 28, and Other (any other area). _This variable is used as a proxy for urban and socioeconomic context, since different areas can reflect different passenger profiles._\n",
    "\n",
    "- Duration_group: Built from the duration_minutes variable by applying percentile-based binning with the following ranges: Flash Riders ≤10 min, Urban Cruisers between 10 and 23.2 min, and Long-Haul Nomads >23.2 min. _This grouping reflects different types of trips, from short rides typical of urban centers to long journeys, each with different expectations and behaviors associated with the service._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These three variables define the partitioning space, generating combinations that capture different passenger profiles. In total, the result is:\n",
    "\n",
    "- 4 (payment_group) × 5 (pickup_zone_group) × 3 (duration_group) = **60 partition combinations**\n",
    "\n",
    "All these combinations describe a wide range of travelers, which inevitably leads to combinations that occur only rarely. To reduce the complexity of our problem, partitions that occur less than 2% of the time within our dataset are merged, reducing the number of combinations to 21 (including the newly merged group).\n",
    "\n",
    "Each of these partitions represents a distinct travel profile (for example, long trips paid with a card and starting in tourist areas). It is important to emphasize that not all profiles have the same proportion of data, which could introduce bias if the sampling technique is not properly defined. For this reason, stratified sampling was chosen, as it allows extracting a balanced proportion of records across all groups, preventing the model from learning patterns solely from the most frequent profiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **References**\n",
    "Smartcitiesworld. (2022). Predictive Analytics Key to Easing Traffic Congestion.  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; https: //www.smartcitiesworld.net/news/news/predictive-analytics-key-to-aeasing-traffic-congestion-7502\n",
    "\n",
    "Guo, Y., Liu, Y., Wang, J., & Chen, H. (2023). Urban Mobility Hotspots and Their Implications for Resilient City Planning.  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; journal of transport geography, 108, 103567. https://www.sciencedirect.com/science/article/pii/s096669232300039x?via%3dihub\n",
    "\n",
    "LE, James. (2019, July 23). Using Ant Colony and Genetic Evolution to Optimize Ride-Sharing Trip Duration.  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; medium. https://medium.com/data-science/using-ant-colony-and-genetic-evolution-to-optimize-ride-sharing-trip-duration-56194215923F\n",
    "\n",
    "City of Chicago. (2024). Taxi Trips (2024-) [Data set].  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; https: //data.cityofchicago.org/transportation/taxi-trips-2024-/ajtu-isnz/about_data\n",
    "\n",
    "Ahmed, S. K. (2024). Research Methodology Simplified: How to Choose The Right Sample Technique and Determine The Approprot Sample Size for Research.  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; oral oncology reports, 12, 100662. https://doi.org/10.1016/J.OOR.2024.100662\n",
    "\n",
    "Pok, A. (2023). Scaling Machine Learning with Spark: Distributed ml with mllib, tensorflow, and pytorch. O’Reilly Media.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbbgE8X5mIeA"
   },
   "source": [
    "## **Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcCnhJwyiTyu"
   },
   "source": [
    "#### **Library Import**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKlyBEbdmIeA"
   },
   "source": [
    "As a first step, we import the libraries required for the execution of our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XXKGZB4SiQjr"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Imputer,StringIndexer,OneHotEncoder,StandardScaler,VectorAssembler\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import col, isnan, when, count, percentile_approx, min, max, mean, stddev, approx_count_distinct, expr,  concat_ws, lit\n",
    "from pyspark.sql.functions import hour, dayofweek, unix_timestamp, when, month,to_timestamp,  dayofweek\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, ClusteringEvaluator\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LS8Q2W_nioUA"
   },
   "source": [
    "#### **Creating the Spark Session**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkFkemldmIeD"
   },
   "source": [
    "Next, we create our Spark session and define a function that will allow us to display Spark DataFrames in a more user-friendly way. To do this, the DataFrame is converted into a pandas DataFrame and rendered using HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eEuzkb88ism7"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ChicagoTaxyTripsAnalysis\") \\\n",
    "     .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.python.worker.timeout\", \"600\") \\\n",
    "    .config(\"spark.python.worker.retries\", \"3\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Wqfr5kbVmIeE"
   },
   "outputs": [],
   "source": [
    "def pretty_display(df, limit=100):\n",
    "    pdf = df.limit(limit).toPandas()\n",
    "    display(HTML(pdf.to_html(notebook=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzhfCehBi5hF"
   },
   "source": [
    "#### **Loading the Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rI6EI7vVmIeF"
   },
   "source": [
    "We now load our dataset and check the number of rows and columns, giving us an idea of the dataset’s size and dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q81a5PRN5X-I",
    "outputId": "a1f5f14c-68ed-4b1a-a31f-1c5367e55d37"
   },
   "outputs": [],
   "source": [
    "filename = \"Taxi_Trips__2024-__20250426.csv\"\n",
    "# Local route\n",
    "local_path = f\"{filename}\"\n",
    "\n",
    "dftaxytrips = spark.read.csv(local_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9j_i4-qkfh4",
    "outputId": "ba59d20c-481e-4fd7-a0b6-7a9fcd613467"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 7917844\n",
      "Number of columns: 23\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of records:\", dftaxytrips.count())\n",
    "print(\"Number of columns:\", len(dftaxytrips.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idr7YdRflQJn"
   },
   "source": [
    "#### **Data Exploration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHOTbsh8mIeH"
   },
   "source": [
    "Next, we examine the schema of our dataset, paying attention to columns that are of textual type, as these will require different handling in later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIvycSZ_lQxt",
    "outputId": "e9d4a00a-42fb-4b7f-e114-2d41146f69e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Trip ID: string (nullable = true)\n",
      " |-- Taxi ID: string (nullable = true)\n",
      " |-- Trip Start Timestamp: string (nullable = true)\n",
      " |-- Trip End Timestamp: string (nullable = true)\n",
      " |-- Trip Seconds: integer (nullable = true)\n",
      " |-- Trip Miles: double (nullable = true)\n",
      " |-- Pickup Census Tract: long (nullable = true)\n",
      " |-- Dropoff Census Tract: long (nullable = true)\n",
      " |-- Pickup Community Area: integer (nullable = true)\n",
      " |-- Dropoff Community Area: integer (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tips: double (nullable = true)\n",
      " |-- Tolls: double (nullable = true)\n",
      " |-- Extras: double (nullable = true)\n",
      " |-- Trip Total: double (nullable = true)\n",
      " |-- Payment Type: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Pickup Centroid Latitude: double (nullable = true)\n",
      " |-- Pickup Centroid Longitude: double (nullable = true)\n",
      " |-- Pickup Centroid Location: string (nullable = true)\n",
      " |-- Dropoff Centroid Latitude: double (nullable = true)\n",
      " |-- Dropoff Centroid Longitude: double (nullable = true)\n",
      " |-- Dropoff Centroid  Location: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dftaxytrips.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mj5F8yzJujId"
   },
   "source": [
    "#### **Characterization Variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wc65mjBxmIeH"
   },
   "source": [
    "As mentioned in previous deliveries, we decided to apply transformations to our data to generate new columns that provide meaningful information for our analysis. A clear example of this is the creation of the columns \"trip_hour\", \"trip_day_of_week\", and \"trip_month\", which extract components from the trip’s start date and time. These columns can be very important in identifying patterns in our data.\n",
    "\n",
    "In addition, some columns are grouped into categories, and binning is applied to others to reduce the complexity of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pUk-nGDRK8-b"
   },
   "outputs": [],
   "source": [
    "# Trip Start Timestamp is Timestamp type\n",
    "dftaxytrips = dftaxytrips.withColumn(\n",
    "    \"trip_start_ts\",\n",
    "    to_timestamp(col(\"Trip Start Timestamp\"), \"MM/dd/yyyy hh:mm:ss a\")\n",
    ")\n",
    "\n",
    "# Time of day\n",
    "dftaxytrips = dftaxytrips.withColumn(\"trip_hour\", hour(col(\"trip_start_ts\")))\n",
    "\n",
    "# Week day (1 = Sunday, 7 = Saturday)\n",
    "dftaxytrips = dftaxytrips.withColumn(\"trip_day_of_week\", dayofweek(col(\"trip_start_ts\")))\n",
    "\n",
    "# Month of the year (January 1, 12 = December)\n",
    "dftaxytrips = dftaxytrips.withColumn(\"trip_month\", month(col(\"trip_start_ts\")))\n",
    "\n",
    "# Trip duration in minutes\n",
    "dftaxytrips = dftaxytrips.withColumn(\"duration_minutes\", col(\"Trip Seconds\") / 60)\n",
    "\n",
    "# Tip/Fare ratio\n",
    "dftaxytrips = dftaxytrips.withColumn(\"tip_ratio\",\n",
    "    when(col(\"Fare\") > 0, col(\"Tips\") / col(\"Fare\")).otherwise(0))\n",
    "\n",
    "# Tip/Trip Miles Ratio\n",
    "dftaxytrips = dftaxytrips.withColumn(\"tip_per_mile\",\n",
    "    when(col(\"Trip Miles\") > 0, col(\"Tips\") / col(\"Trip Miles\")).otherwise(0))\n",
    "\n",
    "# Payment method group\n",
    "dftaxytrips = dftaxytrips.withColumn(\"payment_group\",\n",
    "    when(col(\"Payment Type\") == \"Credit Card\", \"Credit Card\")\n",
    "    .when(col(\"Payment Type\") == \"Cash\", \"Cash\")\n",
    "    .when(col(\"Payment Type\") == \"Mobile\", \"Mobile\")\n",
    "    .otherwise(\"Other\"))\n",
    "\n",
    "# Company group\n",
    "dftaxytrips = dftaxytrips.withColumn(\"company_group\",\n",
    "    when(col(\"Company\") == \"Flash Cab\", \"Flash Cab\")\n",
    "    .when(col(\"Company\") == \"Taxi Affiliation Services\", \"Taxi Affiliation\")\n",
    "    .when(col(\"Company\") == \"Taxicab Insurance Agency Llc\", \"Insurance Agency\")\n",
    "    .when(col(\"Company\") == \"Sun Taxi\", \"Sun Taxi\")\n",
    "    .when(col(\"Company\") == \"City Service\", \"City Service\")\n",
    "    .when(col(\"Company\") == \"Chicago Independents\", \"Chicago Independents\")\n",
    "    .otherwise(\"Other\"))\n",
    "\n",
    "# Origin Zone Group\n",
    "dftaxytrips = dftaxytrips.withColumn(\"pickup_zone_group\",\n",
    "    when(col(\"Pickup Community Area\") == 76, 76)\n",
    "    .when(col(\"Pickup Community Area\") == 8, 8)\n",
    "    .when(col(\"Pickup Community Area\") == 32, 32)\n",
    "    .when(col(\"Pickup Community Area\") == 28, 28)\n",
    "    .otherwise(\"Other\"))\n",
    "\n",
    "# Group Destination Zone\n",
    "dftaxytrips = dftaxytrips.withColumn(\"dropoff_zone_group\",\n",
    "    when(col(\"Dropoff Community Area\") == 8, 8)\n",
    "    .when(col(\"Dropoff Community Area\") == 32, 32)\n",
    "    .when(col(\"Dropoff Community Area\") == 28, 28)\n",
    "    .when(col(\"Dropoff Community Area\") == 76, 76)\n",
    "    .otherwise(\"Other\"))\n",
    "\n",
    "# Rename certain columns\n",
    "dftaxytrips = dftaxytrips.withColumnRenamed(\"Trip ID\", \"trip_id\")\n",
    "dftaxytrips = dftaxytrips.withColumnRenamed(\"Trip Miles\", \"trip_miles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LoFhkJOFAGRf"
   },
   "outputs": [],
   "source": [
    "# Trip duration (in minutes)\n",
    "dftaxytrips = dftaxytrips.withColumn(\n",
    "    \"duration_group\",\n",
    "    (\n",
    "        when(col(\"duration_minutes\") <= 10.0, \"Flash Riders\")           # Very short, high -rotation trips\n",
    "        .when(col(\"duration_minutes\") <= 23.2, \"Urban Cruisers\")        # Typical paths within the city\n",
    "        .otherwise(\"Long-Haul Nomads\")                                  # long journeys, possibly between distant districts\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzRNIGDVIqR9"
   },
   "source": [
    "### **Data Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8p1No4yW29ql"
   },
   "source": [
    "#### **Partitioning Rules**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kibtyqeSDksz"
   },
   "source": [
    "Since the main goal of the project is to analyze tips in Chicago taxi trips and predict relevant patterns, we have chosen the following three variables for partitioning:\n",
    "\n",
    "\n",
    "* **`Payment_group`:** Groups payment methods into Credit Card, Cash, Mobile, and Other. This variable is considered key due to the strong relationship between card payments and tipping behavior.\n",
    "* **`Pickup_zone_group`:** Groups pickup zones into specific areas (76, 8, 32, 28) and an “Other” group that includes all remaining zones. It serves as a proxy for socioeconomic or commercial location.\n",
    "* **`duration_group`:** Classifies trip duration into Flash Riders (≤10 min), Urban Cruisers (10–23.2 min), and Long-Haul Nomads (>23.2 min). This grouping captures the intensity and context of the trip.\n",
    "\n",
    "These variables allow us to capture key behavioral factors related to the passenger’s decision to leave a tip.\n",
    "\n",
    "We then partition the dataset using these variables, which will show us the proportion of each segment and help us make informed decisions for our sampling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u65XAmYv0X6W",
    "outputId": "4191861d-ed92-4b45-a01a-cbc4cda61d75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+----------------+------+--------------------+\n",
      "|payment_group|pickup_zone_group|  duration_group| count|          proportion|\n",
      "+-------------+-----------------+----------------+------+--------------------+\n",
      "|  Credit Card|               76|Long-Haul Nomads|932993| 0.11783422355883748|\n",
      "|        Other|            Other|  Urban Cruisers|447407| 0.05650616506210529|\n",
      "|        Other|            Other|Long-Haul Nomads|430338| 0.05435040144766681|\n",
      "|         Cash|            Other|    Flash Riders|310727| 0.03924389012968682|\n",
      "|         Cash|                8|    Flash Riders|306484| 0.03870801192849973|\n",
      "|  Credit Card|               32|    Flash Riders|291907|0.036866980455790746|\n",
      "|  Credit Card|                8|    Flash Riders|276498| 0.03492086987316244|\n",
      "|         Cash|               32|    Flash Riders|255828| 0.03231031073610442|\n",
      "|       Mobile|                8|    Flash Riders|225234|0.028446380100441485|\n",
      "|  Credit Card|            Other|Long-Haul Nomads|211508|0.026712827380786994|\n",
      "|         Cash|               76|Long-Haul Nomads|206361|0.026062776685168335|\n",
      "|         Cash|            Other|Long-Haul Nomads|205220| 0.02591867180005062|\n",
      "|       Mobile|            Other|  Urban Cruisers|203128|0.025654458461166953|\n",
      "|  Credit Card|               76|  Urban Cruisers|199533|  0.0252004207205901|\n",
      "|         Cash|            Other|  Urban Cruisers|194488| 0.02456325231969713|\n",
      "|  Credit Card|                8|  Urban Cruisers|183293|0.023149357325049597|\n",
      "|       Mobile|                8|  Urban Cruisers|179042| 0.02261246874780559|\n",
      "|  Credit Card|            Other|  Urban Cruisers|162120|0.020475270793412954|\n",
      "|         Cash|                8|  Urban Cruisers|159390|0.020130479963990196|\n",
      "|  Credit Card|               28|    Flash Riders|158992|0.020080213755158602|\n",
      "|  Credit Card|               32|  Urban Cruisers|154174|0.019471714774880636|\n",
      "|         Cash|               28|    Flash Riders|142729|0.018026245528454464|\n",
      "|  Credit Card|            Other|    Flash Riders|138692|0.017516384510732973|\n",
      "|       Mobile|               32|    Flash Riders|126880|0.016024564262695756|\n",
      "|       Mobile|               32|  Urban Cruisers|100057| 0.01263689964086183|\n",
      "|       Mobile|            Other|Long-Haul Nomads| 99085|0.012514138949946475|\n",
      "|         Cash|               32|  Urban Cruisers| 97494|0.012313200411627206|\n",
      "|       Mobile|               76|Long-Haul Nomads| 94029|0.011875581281975245|\n",
      "|  Credit Card|               28|  Urban Cruisers| 93965|0.011867498273519912|\n",
      "|  Credit Card|               32|Long-Haul Nomads| 86405|0.010912692899733817|\n",
      "|       Mobile|            Other|    Flash Riders| 85058|  0.0107425708311505|\n",
      "|  Credit Card|                8|Long-Haul Nomads| 84164|0.010629661306790082|\n",
      "|        Other|            Other|    Flash Riders| 82513|0.010421144948043937|\n",
      "|         Cash|               28|  Urban Cruisers| 80382|0.010152006025882803|\n",
      "|  Credit Card|               76|    Flash Riders| 74094|0.009357850445146431|\n",
      "|         Cash|               76|  Urban Cruisers| 71610|0.009044128679473858|\n",
      "|        Other|                8|  Urban Cruisers| 70825|0.008944985528888925|\n",
      "|         Cash|               76|    Flash Riders| 69526|0.008780925716647108|\n",
      "|       Mobile|               28|  Urban Cruisers| 67629|0.008541340294150782|\n",
      "|       Mobile|               28|    Flash Riders| 55853|0.007054066738369688|\n",
      "|        Other|                8|Long-Haul Nomads| 55355|0.006991170828826...|\n",
      "|         Cash|                8|Long-Haul Nomads| 53054|0.006700561415456025|\n",
      "|        Other|               28|  Urban Cruisers| 45480|0.005743987883570325|\n",
      "|       Mobile|                8|Long-Haul Nomads| 43276|0.005465629279889829|\n",
      "|        Other|               28|Long-Haul Nomads| 41306|0.005216824175874139|\n",
      "|        Other|               32|  Urban Cruisers| 35917|0.004536209604533759|\n",
      "|         Cash|               32|Long-Haul Nomads| 35216|0.004447675402546451|\n",
      "|        Other|               32|Long-Haul Nomads| 33669|0.004252293932540222|\n",
      "|       Mobile|               32|Long-Haul Nomads| 27848|0.003517119054126...|\n",
      "|         Cash|               28|Long-Haul Nomads| 25386|0.003206175822610...|\n",
      "|        Other|                8|    Flash Riders| 22324|0.002819454386825...|\n",
      "|  Credit Card|               28|Long-Haul Nomads| 17816|0.002250107478753...|\n",
      "|       Mobile|               76|  Urban Cruisers| 15166|0.001915420409899...|\n",
      "|       Mobile|               28|Long-Haul Nomads| 14459|0.001826128425869...|\n",
      "|        Other|               32|    Flash Riders|  9440|0.001192243747161...|\n",
      "|        Other|               76|Long-Haul Nomads|  9007|0.001137557143080869|\n",
      "|        Other|               28|    Flash Riders|  8300|0.001048265159050873|\n",
      "|        Other|               76|  Urban Cruisers|  5087| 6.42472875191782E-4|\n",
      "|        Other|               76|    Flash Riders|  2060|2.601718346559997...|\n",
      "|       Mobile|               76|    Flash Riders|  2023|2.554988453927609...|\n",
      "+-------------+-----------------+----------------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "partition_counts = dftaxytrips.groupBy(\n",
    "    \"payment_group\", \"pickup_zone_group\", \"duration_group\"\n",
    ").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Calculate total general\n",
    "total_count = dftaxytrips.count()\n",
    "\n",
    "# Add proportion by combination\n",
    "partition_counts = partition_counts.withColumn(\n",
    "    \"proportion\", col(\"count\") / total_count\n",
    ")\n",
    "\n",
    "# Orderly\n",
    "partition_counts.orderBy(col(\"proportion\").desc()).show(60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNWVxV-4mIeJ"
   },
   "source": [
    "As we can see, there are a total of 60 segments in our population. Many of these have a very small number of records, so including them in our sampling techniques could lead to errors and unnecessarily complicate the process with segments that are not representative of the overall population. For these reasons, we chose to group all segments that represent less than 2% of the population into a new segment labeled \"Other.\"\n",
    "\n",
    "To achieve this, the following steps are carried out:\n",
    "- A dataset is created containing only the segments with more than 2% of the population, using a filter on the \"proportion\" column.\n",
    "- A new column, \"StrataGrouping\", is added to this dataset. It serves as an identifier by concatenating the values of \"payment_group\", \"pickup_zone_group\", and \"duration_group\".\n",
    "- This dataset is then joined with the original dataset using the three aforementioned columns. As a result, the \"StrataGrouping\" column will have values only for the segments with more than 2% of the population.\n",
    "- For the remaining segments, the value \"Other\" is imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MPRRBPU2mIeJ"
   },
   "outputs": [],
   "source": [
    "significant_combinations = partition_counts.filter(col(\"proportion\") > 0.02)\n",
    "\n",
    "significant_combinations = significant_combinations.withColumn(\n",
    "    \"StrataGrouping\", concat_ws(\"_\", \"payment_group\", \"pickup_zone_group\", \"duration_group\")\n",
    ")\n",
    "\n",
    "dftaxytrips_with_strata = dftaxytrips.join(\n",
    "    significant_combinations.select(\n",
    "        \"payment_group\", \"pickup_zone_group\", \"duration_group\", \"StrataGrouping\"\n",
    "    ),\n",
    "    on=[\"payment_group\", \"pickup_zone_group\", \"duration_group\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "dftaxytrips_with_strata = dftaxytrips_with_strata.withColumn(\n",
    "    \"StrataGrouping\",\n",
    "    when(col(\"StrataGrouping\").isNull(), lit(\"Other\")).otherwise(col(\"StrataGrouping\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8upul1MnmIeJ",
    "outputId": "b96679d4-43ff-4820-ae42-b24290a47c2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+\n",
      "|      StrataGrouping|  count|          proportion|\n",
      "+--------------------+-------+--------------------+\n",
      "|               Other|2377353|  0.3002525687548277|\n",
      "|Credit Card_76_Lo...| 932993| 0.11783422355883748|\n",
      "|Other_Other_Urban...| 447407| 0.05650616506210529|\n",
      "|Other_Other_Long-...| 430338| 0.05435040144766681|\n",
      "|Cash_Other_Flash ...| 310727| 0.03924389012968682|\n",
      "| Cash_8_Flash Riders| 306484| 0.03870801192849973|\n",
      "|Credit Card_32_Fl...| 291907|0.036866980455790746|\n",
      "|Credit Card_8_Fla...| 276498| 0.03492086987316244|\n",
      "|Cash_32_Flash Riders| 255828| 0.03231031073610442|\n",
      "|Mobile_8_Flash Ri...| 225234|0.028446380100441485|\n",
      "|Credit Card_Other...| 211508|0.026712827380786994|\n",
      "|Cash_76_Long-Haul...| 206361|0.026062776685168335|\n",
      "|Cash_Other_Long-H...| 205220| 0.02591867180005062|\n",
      "|Mobile_Other_Urba...| 203128|0.025654458461166953|\n",
      "|Credit Card_76_Ur...| 199533|  0.0252004207205901|\n",
      "|Cash_Other_Urban ...| 194488| 0.02456325231969713|\n",
      "|Credit Card_8_Urb...| 183293|0.023149357325049597|\n",
      "|Mobile_8_Urban Cr...| 179042| 0.02261246874780559|\n",
      "|Credit Card_Other...| 162120|0.020475270793412954|\n",
      "|Cash_8_Urban Crui...| 159390|0.020130479963990196|\n",
      "|Credit Card_28_Fl...| 158992|0.020080213755158602|\n",
      "+--------------------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_partition_counts = dftaxytrips_with_strata.groupBy(\n",
    "    \"StrataGrouping\"\n",
    ").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Add proportion by combination\n",
    "grouped_partition_counts = grouped_partition_counts.withColumn(\n",
    "    \"proportion\", col(\"count\") / total_count\n",
    ")\n",
    "\n",
    "# Orderly\n",
    "grouped_partition_counts.orderBy(col(\"proportion\").desc()).show(60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RaPEv3N3B-k"
   },
   "source": [
    "#### **Sampling Technique**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42TMmT_PmIeK"
   },
   "source": [
    "Now that we have the \"StrataGrouping\" column, we can use stratified sampling.\n",
    "To do this, we first create a dictionary that takes the unique values of \"StrataGrouping\" and assigns them the percentage of values that will be taken from the population of that segment. Then, the sampleBy function is used to extract the data.\n",
    "\n",
    "In our case, we chose to use 15% of the data from each stratum. This allows us to save a considerable amount of processing and storage resources, while preserving the data distribution without affecting any stratum, maintaining the integrity of our population within the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "yALr8VZumIeK"
   },
   "outputs": [],
   "source": [
    "fractions_df = dftaxytrips_with_strata.select(\"StrataGrouping\").distinct().withColumn(\"fraction\",lit(0.15))\n",
    "fractions_dict = fractions_df.rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zgDEvsepmIeK"
   },
   "outputs": [],
   "source": [
    "sampled_df = dftaxytrips_with_strata.stat.sampleBy(\"StrataGrouping\", fractions_dict, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "gSHJOnTlmIeK"
   },
   "outputs": [],
   "source": [
    "sampled_total_count = sampled_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qv7VxOgSmIeL",
    "outputId": "6bd175fa-7eb7-447f-b97e-9aac2c6b437b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+\n",
      "|      StrataGrouping| count|          proportion|\n",
      "+--------------------+------+--------------------+\n",
      "|               Other|356854| 0.30022976553146846|\n",
      "|Credit Card_76_Lo...|140422| 0.11814037151176633|\n",
      "|Other_Other_Urban...| 67046| 0.05640739590931539|\n",
      "|Other_Other_Long-...| 64443| 0.05421743004182221|\n",
      "|Cash_Other_Flash ...| 46794| 0.03936890618650635|\n",
      "| Cash_8_Flash Riders| 46027| 0.03872361082716433|\n",
      "|Credit Card_32_Fl...| 44029|0.037042645862411586|\n",
      "|Credit Card_8_Fla...| 41543| 0.03495111488024176|\n",
      "|Cash_32_Flash Riders| 38282| 0.03220755794828046|\n",
      "|Mobile_8_Flash Ri...| 33494|  0.0281792995642784|\n",
      "|Credit Card_Other...| 31932|0.026865151779021254|\n",
      "|Cash_Other_Long-H...| 30787| 0.02590183602094223|\n",
      "|Mobile_Other_Urba...| 30682|0.025813497021293066|\n",
      "|Cash_76_Long-Haul...| 30650|0.025786574659495222|\n",
      "|Credit Card_76_Ur...| 29751|0.025030224557737107|\n",
      "|Cash_Other_Urban ...| 29284|0.024637326340249857|\n",
      "|Credit Card_8_Urb...| 27575|0.023199503955483876|\n",
      "|Mobile_8_Urban Cr...| 26774| 0.02252560358673165|\n",
      "|Cash_8_Urban Crui...| 24145| 0.02031376330027772|\n",
      "|Credit Card_Other...| 24096|0.020272538433774776|\n",
      "|Credit Card_28_Fl...| 23993|0.020185882081737973|\n",
      "+--------------------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampled_partition_counts = sampled_df.groupBy(\n",
    "    \"StrataGrouping\"\n",
    ").agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Add proportion per combination\n",
    "sampled_partition_counts = sampled_partition_counts.withColumn(\n",
    "    \"proportion\", col(\"count\") / sampled_total_count\n",
    ")\n",
    "\n",
    "# Sort by the most representative\n",
    "sampled_partition_counts.orderBy(col(\"proportion\").desc()).show(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pYxWdXwtmIeL",
    "outputId": "ceadf3ac-2f93-4b55-ae1d-1cff7afde9b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1188603\n"
     ]
    }
   ],
   "source": [
    "print(sampled_total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yoQp5fkNmIeL",
    "outputId": "ddb22844-e480-496f-c6b2-365671a4e01f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7917844\n"
     ]
    }
   ],
   "source": [
    "print(total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BU_phiEbmIeL"
   },
   "source": [
    "### **Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjjM3Jq6mIeL"
   },
   "source": [
    "##### **Elimination of non-important columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DVk7vtbMmIeM",
    "outputId": "7a85410c-857f-4036-d922-c03ffa3bb1c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- payment_group: string (nullable = false)\n",
      " |-- pickup_zone_group: string (nullable = false)\n",
      " |-- duration_group: string (nullable = false)\n",
      " |-- trip_id: string (nullable = true)\n",
      " |-- Taxi ID: string (nullable = true)\n",
      " |-- Trip Start Timestamp: string (nullable = true)\n",
      " |-- Trip End Timestamp: string (nullable = true)\n",
      " |-- Trip Seconds: integer (nullable = true)\n",
      " |-- trip_miles: double (nullable = true)\n",
      " |-- Pickup Census Tract: long (nullable = true)\n",
      " |-- Dropoff Census Tract: long (nullable = true)\n",
      " |-- Pickup Community Area: integer (nullable = true)\n",
      " |-- Dropoff Community Area: integer (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tips: double (nullable = true)\n",
      " |-- Tolls: double (nullable = true)\n",
      " |-- Extras: double (nullable = true)\n",
      " |-- Trip Total: double (nullable = true)\n",
      " |-- Payment Type: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Pickup Centroid Latitude: double (nullable = true)\n",
      " |-- Pickup Centroid Longitude: double (nullable = true)\n",
      " |-- Pickup Centroid Location: string (nullable = true)\n",
      " |-- Dropoff Centroid Latitude: double (nullable = true)\n",
      " |-- Dropoff Centroid Longitude: double (nullable = true)\n",
      " |-- Dropoff Centroid  Location: string (nullable = true)\n",
      " |-- trip_start_ts: timestamp (nullable = true)\n",
      " |-- trip_hour: integer (nullable = true)\n",
      " |-- trip_day_of_week: integer (nullable = true)\n",
      " |-- trip_month: integer (nullable = true)\n",
      " |-- duration_minutes: double (nullable = true)\n",
      " |-- tip_ratio: double (nullable = true)\n",
      " |-- tip_per_mile: double (nullable = true)\n",
      " |-- company_group: string (nullable = false)\n",
      " |-- dropoff_zone_group: string (nullable = false)\n",
      " |-- StrataGrouping: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DATASET structure\n",
    "sampled_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3dWn0D_mIeM"
   },
   "source": [
    "In our dataset we can observe that there are multiple columns dedicated to representing location data, this complicates our model without providing significant information, so we will eliminate all except 'pickup_zone_group' and 'Drugff_zone_group', which we build previously for this same reason.\n",
    "\n",
    "In addition, the 'Trip Minute' column was calculated directly from the 'Trip Seconds' column, so we eliminate the latter to avoid collinearity problems. The 'trip_id' and 'taxi_id' columns do not present important values for our analysis, so they can be eliminated. And the columns 'Trip Start Timestamp', 'Trip End Timestamp' and 'Trip_start_ts' present a problem similar to the previous ones, where their values are represented in other columns or do not contribute to important information for our analysis, so we can eliminate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "d5IgAxWBmIeM"
   },
   "outputs": [],
   "source": [
    "sampled_df = sampled_df.drop('Pickup Census Tract','Dropoff Census Tract','Pickup Community Area','Dropoff Community Area','Pickup Centroid Longitude','Pickup Centroid Latitude','Dropoff Centroid Latitude','Pickup Centroid Location','Dropoff Centroid Longitude','Dropoff Centroid  Location','Taxi ID','trip_id','Trip Seconds','Trip Start Timestamp', 'Trip End Timestamp','trip_start_ts','Payment Type','Company','StrataGrouping')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iNOQfhqmIeM"
   },
   "source": [
    "##### **Handling Missing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "5KoGo71CmIeM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_taxytrips = sampled_df.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c)\n",
    "    for c in sampled_df.columns\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99
    },
    "id": "jUeKdADMmIeN",
    "outputId": "54a5b3bc-c067-48eb-f2e4-90822656af4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in our sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_group</th>\n",
       "      <th>pickup_zone_group</th>\n",
       "      <th>duration_group</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Tips</th>\n",
       "      <th>Tolls</th>\n",
       "      <th>Extras</th>\n",
       "      <th>Trip Total</th>\n",
       "      <th>trip_hour</th>\n",
       "      <th>trip_day_of_week</th>\n",
       "      <th>trip_month</th>\n",
       "      <th>duration_minutes</th>\n",
       "      <th>tip_ratio</th>\n",
       "      <th>tip_per_mile</th>\n",
       "      <th>company_group</th>\n",
       "      <th>dropoff_zone_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3049</td>\n",
       "      <td>3049</td>\n",
       "      <td>3049</td>\n",
       "      <td>3049</td>\n",
       "      <td>3049</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>244</td>\n",
       "      <td>0</td>\n",
       "      <td>2790</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Missing values in our sample:\")\n",
    "pretty_display(missing_taxytrips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2z0yK_I-mIeN"
   },
   "source": [
    "Because our goal is to predict the TIP values, it is important to eliminate those cases in which this column has null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "AIs2caDjmIeN"
   },
   "outputs": [],
   "source": [
    "sampled_df = sampled_df.where(sampled_df.Tips != 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_Elpii-mIeN"
   },
   "source": [
    "For the remaining numerical columns we can define a simple imputator that uses the average of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "E3WtZ2j4mIeO"
   },
   "outputs": [],
   "source": [
    "# List of numerical variables to impute\n",
    "vars_a_imputar = [\"duration_minutes\", \"trip_miles\", \"Fare\",\"Tolls\",\"Extras\",\"Trip Total\",\"tip_ratio\", \"tip_per_mile\"]\n",
    "\n",
    "# We apply imputation with median\n",
    "for var in vars_a_imputar:\n",
    "    mediana = sampled_df.approxQuantile(var, [0.5], 0.01)[0]\n",
    "    sampled_df = sampled_df.withColumn(\n",
    "        var, when(col(var).isNull(), mediana).otherwise(col(var))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EAZAYzsmIeO"
   },
   "source": [
    "For categorical variables, we define an imputer that uses a placeholder value to ensure that no null values remain in the future.\n",
    "\n",
    "Meanwhile, the variables \"trip_hour\", \"trip_day_of_week\", and \"trip_month\" are numeric types, but they represent categorical values (hour, day of the week, and month). Therefore, I will treat them as categorical variables and assign them a separate imputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "K07RHOe4mIeO"
   },
   "outputs": [],
   "source": [
    "# List of numerical variables to impute\n",
    "vars_a_imputar = [\"payment_group\", \"pickup_zone_group\", \"duration_group\",\"company_group\",\"dropoff_zone_group\"]\n",
    "\n",
    "# We apply imputation with median\n",
    "for var in vars_a_imputar:\n",
    "    sampled_df = sampled_df.withColumn(\n",
    "        var, when(col(var).isNull(), \"NA\").otherwise(col(var))\n",
    "    )\n",
    "vars_a_imputar = [\"trip_hour\",\"trip_day_of_week\",\"trip_month\"]\n",
    "\n",
    "# We apply imputation with median\n",
    "for var in vars_a_imputar:\n",
    "    sampled_df = sampled_df.withColumn(\n",
    "        var, when(col(var).isNull(), 0).otherwise(col(var))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "gJL6a4bcmIeO"
   },
   "outputs": [],
   "source": [
    "# Analysis of missing values in 'dftaxytrips_selected'\n",
    "missing_taxytrips = sampled_df.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c)\n",
    "    for c in sampled_df.columns\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 99
    },
    "id": "BrDPBvtkmIeO",
    "outputId": "a5bac6a4-c66c-417c-9d29-3fe4e25086ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in Chicago Taxi Trips Dataset (CSV):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>payment_group</th>\n",
       "      <th>pickup_zone_group</th>\n",
       "      <th>duration_group</th>\n",
       "      <th>trip_miles</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Tips</th>\n",
       "      <th>Tolls</th>\n",
       "      <th>Extras</th>\n",
       "      <th>Trip Total</th>\n",
       "      <th>trip_hour</th>\n",
       "      <th>trip_day_of_week</th>\n",
       "      <th>trip_month</th>\n",
       "      <th>duration_minutes</th>\n",
       "      <th>tip_ratio</th>\n",
       "      <th>tip_per_mile</th>\n",
       "      <th>company_group</th>\n",
       "      <th>dropoff_zone_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Missing values in Chicago Taxi Trips Dataset (CSV):\")\n",
    "pretty_display(missing_taxytrips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DjXkA1xfmIeP",
    "outputId": "e87481a3-db6e-43c4-d378-f62da90d43dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- payment_group: string (nullable = false)\n",
      " |-- pickup_zone_group: string (nullable = false)\n",
      " |-- duration_group: string (nullable = false)\n",
      " |-- trip_miles: double (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Tips: double (nullable = true)\n",
      " |-- Tolls: double (nullable = true)\n",
      " |-- Extras: double (nullable = true)\n",
      " |-- Trip Total: double (nullable = true)\n",
      " |-- trip_hour: integer (nullable = true)\n",
      " |-- trip_day_of_week: integer (nullable = true)\n",
      " |-- trip_month: integer (nullable = true)\n",
      " |-- duration_minutes: double (nullable = true)\n",
      " |-- tip_ratio: double (nullable = true)\n",
      " |-- tip_per_mile: double (nullable = true)\n",
      " |-- company_group: string (nullable = false)\n",
      " |-- dropoff_zone_group: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampled_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EFPs0niZmIeP"
   },
   "source": [
    "##### **Handling Atypical Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sg4qvOIVmIeP"
   },
   "source": [
    "First we define auxiliary functions that will help us detect and eliminate atypical values. It is important to consider that even if atypical values are not detected in the current sample, we have to include code to deal with them when they occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "j_iF4bOzmIeP"
   },
   "outputs": [],
   "source": [
    "def count_outliers(df, column):\n",
    "    percentiles = df.approxQuantile(column, [0.25, 0.75], 0.05)\n",
    "    Q1 = percentiles[0]\n",
    "    Q3 = percentiles[1]\n",
    "\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_limit = Q1 - 1.5 * IQR\n",
    "    upper_limit = Q3 + 1.5 * IQR\n",
    "\n",
    "    return df.filter((col(column) < lower_limit) & (col(column) > upper_limit)).count()\n",
    "\n",
    "def remove_outliers_inplace(df, column):\n",
    "\n",
    "    percentiles = df.approxQuantile(column, [0.25, 0.75], 0.05)\n",
    "    Q1 = percentiles[0]\n",
    "    Q3 = percentiles[1]\n",
    "\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_limit = Q1 - 1.5 * IQR\n",
    "    upper_limit = Q3 + 1.5 * IQR\n",
    "\n",
    "    df = df.filter((col(column) >= lower_limit) & (col(column) <= upper_limit))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzS9QhGymIeP",
    "outputId": "33784a71-5606-45a5-8352-eedaa0072068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores atípicos para duration_minutes: 0\n",
      "Valores atípicos para trip_miles: 0\n",
      "Valores atípicos para Fare: 0\n",
      "Valores atípicos para Tips: 0\n",
      "Valores atípicos para Tolls: 0\n",
      "Valores atípicos para Extras: 0\n",
      "Valores atípicos para Trip Total: 0\n",
      "Valores atípicos para tip_ratio: 0\n",
      "Valores atípicos para tip_per_mile: 0\n"
     ]
    }
   ],
   "source": [
    "NumVar = [\"duration_minutes\", \"trip_miles\", \"Fare\",\"Tips\",\"Tolls\",\"Extras\",\"Trip Total\",\"tip_ratio\", \"tip_per_mile\"]\n",
    "for i in NumVar:\n",
    "    outlier_count = count_outliers(sampled_df, i)\n",
    "    print(f\"Valores atípicos para {i}: {outlier_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYFUO1SmmIeQ"
   },
   "source": [
    "We add code that manages atypical values in case they occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Ig7qFGRKmIeQ"
   },
   "outputs": [],
   "source": [
    "sampled_no_outlier_df = sampled_df\n",
    "for i in NumVar:\n",
    "    sampled_no_outlier_df = remove_outliers_inplace(sampled_df, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKnnyAqQmIeQ"
   },
   "source": [
    "### **Preparation of the training and test sets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-a7Ww6u7mIeQ"
   },
   "source": [
    "To avoid issues related to bias injection, the dataset is split into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "5_nNuohCmIeQ"
   },
   "outputs": [],
   "source": [
    "train_df, test_df = sampled_no_outlier_df.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47g8IUq-mIeQ"
   },
   "source": [
    "##### **Categorical data transformation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q97uciKrmIeQ"
   },
   "source": [
    "PySpark models do not allow the use of non-numeric columns, so the following columns must be transformed using a StringIndexer. In addition, one-hot encoding is applied to prevent numeric relationships from being inferred where none exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "utghy2oDmIeR"
   },
   "outputs": [],
   "source": [
    "indexer = StringIndexer(\n",
    "            inputCols=[\"payment_group\",\"company_group\",\"pickup_zone_group\",\"dropoff_zone_group\",\"duration_group\"],\n",
    "            outputCols= [\"payment_group_cat\",\"company_group_cat\",\"pickup_zone_group_cat\",\"dropoff_zone_group_cat\",\"duration_group_cat\"])\n",
    "indexerFit = indexer.fit(train_df)\n",
    "train_indexed_df = indexerFit.transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "kGiK8XwXmIeR"
   },
   "outputs": [],
   "source": [
    "train_indexed_df = train_indexed_df.select(\n",
    " 'trip_miles',\n",
    " 'Fare',\n",
    " 'Tips',\n",
    " 'Tolls',\n",
    " 'Extras',\n",
    " 'Trip Total',\n",
    " 'trip_hour',\n",
    " 'trip_day_of_week',\n",
    " 'trip_month',\n",
    " 'duration_minutes',\n",
    " 'tip_ratio',\n",
    " 'tip_per_mile',\n",
    " 'payment_group_cat',\n",
    " 'company_group_cat',\n",
    " 'pickup_zone_group_cat',\n",
    " 'dropoff_zone_group_cat',\n",
    " 'duration_group_cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "G6XF_hUwmIeR"
   },
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(\n",
    "            inputCols=[\"payment_group_cat\",\"company_group_cat\",\"pickup_zone_group_cat\",\"dropoff_zone_group_cat\",\"duration_group_cat\",\"trip_hour\", \"trip_day_of_week\", \"trip_month\"],\n",
    "            outputCols= [\"payment_group_ohe\",\"company_group_ohe\",\"pickup_zone_group_ohe\",\"dropoff_zone_group_ohe\",\"duration_group_ohe\",\"trip_hour_ohe\", \"trip_day_of_week_ohe\", \"trip_month_ohe\"])\n",
    "encoderFit = encoder.fit(train_indexed_df)\n",
    "train_encoded_df = encoderFit.transform(train_indexed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "KMnMOQ1LmIeR"
   },
   "outputs": [],
   "source": [
    "train_encoded_df = train_encoded_df.select(\n",
    " 'trip_miles',\n",
    " 'Fare',\n",
    " 'Tips',\n",
    " 'Tolls',\n",
    " 'Extras',\n",
    " 'Trip Total',\n",
    " 'trip_hour_ohe',\n",
    " 'trip_day_of_week_ohe',\n",
    " 'trip_month_ohe',\n",
    " 'duration_minutes',\n",
    " 'tip_ratio',\n",
    " 'tip_per_mile',\n",
    " 'payment_group_ohe',\n",
    " 'company_group_ohe',\n",
    " 'pickup_zone_group_ohe',\n",
    " 'dropoff_zone_group_ohe',\n",
    " 'duration_group_ohe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvHB3j0zmIeR"
   },
   "source": [
    "##### **Numerical data transformation for supervised learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBEXJ0rimIeS"
   },
   "source": [
    "Since the numerical data are on different scales and there are no outliers (these were handled in earlier steps), we opted to use a StandardScaler to transform these columns.\n",
    "\n",
    "It is important to note that the Tip column is not modified, as it is our target column for the supervised learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "9DT1DmGemIeS"
   },
   "outputs": [],
   "source": [
    "numeric_cols = [\"duration_minutes\", \"trip_miles\", \"Fare\", \"Tolls\", \"Extras\", \"Trip Total\", \"tip_ratio\", \"tip_per_mile\"]\n",
    "assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"numeric_features_vec\")\n",
    "train_encoded_num_vectorized_df = assembler.transform(train_encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "fp6c7k8OmIeS"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler(\n",
    "    inputCol=\"numeric_features_vec\",\n",
    "    outputCol=\"numeric_features_scaled\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "scalerFit = scaler.fit(train_encoded_num_vectorized_df)\n",
    "train_encoded_num_scaled_df = scalerFit.transform(train_encoded_num_vectorized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiQQgOGImIeS"
   },
   "source": [
    "##### **Final DataFrame for the supervised learning model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTsPeApEmIeT"
   },
   "source": [
    "Finally, the DataFrame for the supervised learning model is created. To achieve this, a VectorAssembler is used to generate the features column, which contains vectors with all the previously transformed columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "V2fOLBYOmIeT"
   },
   "outputs": [],
   "source": [
    "final_features = [\"numeric_features_scaled\", 'payment_group_ohe',\n",
    " 'company_group_ohe',\n",
    " 'pickup_zone_group_ohe',\n",
    " 'dropoff_zone_group_ohe',\n",
    " 'duration_group_ohe','trip_hour_ohe',\n",
    " 'trip_day_of_week_ohe']\n",
    "assembler_final = VectorAssembler(inputCols=final_features, outputCol=\"features\")\n",
    "train_final_df = assembler_final.transform(train_encoded_num_scaled_df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "zO6gIbvMmIeT"
   },
   "outputs": [],
   "source": [
    "train_final_df = train_final_df.select('features','tips')\n",
    "train_final_df = train_final_df.withColumnRenamed(\"tips\", \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ureG0DusmIeT"
   },
   "source": [
    "##### **Transforming the testing set for supervised learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-ieiBR3mIeT"
   },
   "source": [
    "We also transform our test dataset, ensuring that we do not use the fit function on any of our transformers for this set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "S8iqMz5ZmIeU"
   },
   "outputs": [],
   "source": [
    "test_indexed_df = indexerFit.transform(test_df)\n",
    "test_encoded_df = encoderFit.transform(test_indexed_df)\n",
    "test_vectorized_df = assembler.transform(test_encoded_df)\n",
    "test_scaled_df = scalerFit.transform(test_vectorized_df)\n",
    "test_final_df = assembler_final.transform(test_scaled_df)\n",
    "test_final_df = test_final_df.select('features','tips')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "gE8QTYZNmIeU"
   },
   "outputs": [],
   "source": [
    "test_final_df  = test_final_df .withColumnRenamed(\"tips\", \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZNdq2mnmIeV"
   },
   "source": [
    "##### **Numerical data transformation for unsupervised learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoY0BwYjmIeV"
   },
   "source": [
    "The same steps are then repeated for the dataset used in the unsupervised learning model. The main difference here is that this dataset includes the tips column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Zelk98H3mIeV"
   },
   "outputs": [],
   "source": [
    "numeric_cols_nonSup = [\"duration_minutes\", \"trip_miles\", \"Fare\", \"Tolls\", \"Extras\", \"Trip Total\", \"tip_ratio\", \"tip_per_mile\",\"tips\"]\n",
    "assembler_nonSup = VectorAssembler(inputCols=numeric_cols, outputCol=\"numeric_features_vec\")\n",
    "train_vectorized_df_nonSup = assembler.transform(train_encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "7M9_q2YYmIeV"
   },
   "outputs": [],
   "source": [
    "scaler_nonSup = StandardScaler(\n",
    "    inputCol=\"numeric_features_vec\",\n",
    "    outputCol=\"numeric_features_scaled\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "scalerFit_nonSup = scaler.fit(train_vectorized_df_nonSup)\n",
    "train_scaled_df_nonSup = scalerFit.transform(train_vectorized_df_nonSup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6T8hYXamIeV"
   },
   "source": [
    "##### **Final DataFrame for the unsupervised learning model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "KJAhgKzumIeW"
   },
   "outputs": [],
   "source": [
    "train_final_df_nonSup = assembler_final.transform(train_scaled_df_nonSup )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "beruBHFJmIeW"
   },
   "outputs": [],
   "source": [
    "train_final_df_nonSup = train_final_df.select('features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_AcfJt_umIeW"
   },
   "source": [
    "##### **Transforming the testing set for the unsupervised learning model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "uRY7wuATmIeW"
   },
   "outputs": [],
   "source": [
    "test_vectorized_nonSup_df = assembler_nonSup.transform(test_encoded_df)\n",
    "test_scaled_nonSup_df = scalerFit_nonSup.transform(test_vectorized_df)\n",
    "test_final_nonSup_df = assembler_final.transform(test_scaled_df)\n",
    "test_final_nonSup_df = test_final_nonSup_df .select('features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaVC_laFJF1G"
   },
   "source": [
    "### **Building supervised and unsupervised learning models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHE1d2e0mIeW"
   },
   "source": [
    "#### **Creating the supervised learning model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcDBDznBmIeW"
   },
   "source": [
    "For our supervised learning problem, we will use linear regression to predict the values of the 'tips' column.\n",
    "\n",
    "We define our model and the parameters to be used in the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "7op51eQLmIeX"
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.0, 0.01, 0.1]).addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]).build()\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\")\n",
    "cv = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=3,\n",
    "    parallelism=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8ohRYCSmIeX"
   },
   "source": [
    "We train our grid search and obtain the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "95SP8ArumIeY"
   },
   "outputs": [],
   "source": [
    "cv_model = cv.fit(train_final_df)\n",
    "best_model = cv_model.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfDa9E3-mIeY"
   },
   "source": [
    "We print the parameters of our model and evaluate its performance on our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yl1qN82kmIeY",
    "outputId": "baade92c-9ecb-4196-d04b-81801cf1add6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model parameters:\n",
      "  regParam: 0.0\n",
      "  elasticNetParam: 0.5\n",
      "RMSE en set de prueba: 0.3389000723663502\n"
     ]
    }
   ],
   "source": [
    "predictions = best_model.transform(test_final_df)\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Best model parameters:\")\n",
    "print(f\"  regParam: {best_model._java_obj.getRegParam()}\")\n",
    "print(f\"  elasticNetParam: {best_model._java_obj.getElasticNetParam()}\")\n",
    "print(f\"RMSE en set de prueba: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WB9AV-pfmIeZ",
    "outputId": "4f35505b-81c8-407a-ee20-1e15f284ac2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|label|        prediction|\n",
      "+-----+------------------+\n",
      "|  2.0|2.1460191983007038|\n",
      "|  3.0| 3.113884371945795|\n",
      "|  1.0|1.1283108188735556|\n",
      "|  2.0| 2.128203154836537|\n",
      "|  9.5| 9.448396205437547|\n",
      "+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"label\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VjmCX9hBmIeZ"
   },
   "source": [
    "We display metrics for our target column to put our results into context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7vunGTO1mIeZ",
    "outputId": "7b6e5e0c-c546-4311-a2a5-323be06a1999"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+----------+----------+\n",
      "|       mean(label)|    stddev(label)|min(label)|max(label)|\n",
      "+------------------+-----------------+----------+----------+\n",
      "|6.0354008949752345|4.440883954488067|      0.01|     150.0|\n",
      "+------------------+-----------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_final_df.selectExpr(\"mean(label)\", \"stddev(label)\", \"min(label)\", \"max(label)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwaHCvlGEO4L"
   },
   "source": [
    "From this, we can conclude that our model produces strong results that differ only by a few cents when predicting tip amounts, thus achieving our objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihPQE3bimIeZ"
   },
   "source": [
    "#### **Creating the unsupervised learning model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BOxm8remIea"
   },
   "source": [
    "For the unsupervised learning model, we chose to use KMeans to perform clustering on our dataset. This can be very valuable from a business perspective to better understand consumer behavior patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "Z3JfxLFDmIea"
   },
   "outputs": [],
   "source": [
    "k_values = [2, 4, 6, 8]\n",
    "max_iter_values = [10, 20]\n",
    "evaluator = ClusteringEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4m2pV46mIea"
   },
   "source": [
    "We define the values to be used in our GridSearch and the evaluator.\n",
    "We then run the GridSearch; since PySpark does not provide a native implementation, we use two nested for loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "f8aOTZxOmIea"
   },
   "outputs": [],
   "source": [
    "best_model = None\n",
    "best_score = float('-inf')\n",
    "best_params = {}\n",
    "\n",
    "for k in k_values:\n",
    "    for max_iter in max_iter_values:\n",
    "        kmeans = KMeans(k=k, maxIter=max_iter, seed=1)\n",
    "        model = kmeans.fit(train_final_df_nonSup)\n",
    "        predictions = model.transform(train_final_df_nonSup)\n",
    "\n",
    "        score = evaluator.evaluate(predictions)\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_model = model\n",
    "            best_params = {\"k\": k, \"maxIter\": max_iter}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dCxqpxOjmIea",
    "outputId": "6d21730e-3698-4bad-874a-f124929f325e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters={'k': 2, 'maxIter': 10}, Best Result=0.4181930478893609\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Parameters={best_params}, Best Result={best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1naypJuPmIea"
   },
   "source": [
    "We take the best model and try it with our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "P9v0K20fmIea",
    "outputId": "e86d1dda-e6cb-4904-a392-7b28ad76d6a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model results with test data = 0.4313\n"
     ]
    }
   ],
   "source": [
    "testResults = evaluator.evaluate(best_model.transform(test_final_nonSup_df))\n",
    "print(f\"Model results with test data = {testResults:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diwODG1mFGQX"
   },
   "source": [
    "The default evaluation method for the clustering evaluator uses the silhouette score, which ranges from [+1, -1], where 1 indicates an ideal clustering and -1 indicates incorrect clusters.\n",
    "\n",
    "The final evaluation with the test set, resulting in a score of 0.41, indicates that while the clustering is positive, it is not perfect and there is ample room for improvement. Next steps could include a broader hyperparameter search or the use of other algorithms.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
