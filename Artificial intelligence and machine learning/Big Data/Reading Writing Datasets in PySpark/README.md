# ğŸ“‚ Big Data with PySpark: Reading and Writing Files

This project focuses on using **PySpark** to read, write, and explore large datasets as part of the Master in Applied Artificial Intelligence program.

## ğŸš€ Overview

The notebook demonstrates how to:
- Set up a Spark session.
- Read Big Data files using PySpark.
- Write datasets back in different formats.
- Perform initial exploration and manipulation of large datasets.

This exercise introduces core concepts in distributed data processing with PySpark.

## ğŸ“Š Key Steps

1. **Library Imports:** Loading the required libraries for PySpark.
2. **Spark Session:** Creating a Spark session to handle distributed computing.
3. **Loading Data:** Reading datasets into Spark DataFrames from files.
4. **Exploration:** Viewing schema, data types, and sample records.
5. **Writing Data:** Exporting processed datasets into various file formats.

## ğŸ›  Technologies Used

- Python 3.x
- Apache Spark / PySpark
- Jupyter Notebook

## ğŸ‘¨â€ğŸ“ Authors

- Victoria Melgarejo Cabrera - A01795030
- HÃ©ctor Alejandro Alvarez Rosas - A01796262
- Andrea Xcaret Gomez Alfaro - A01796384
- Mario Guillen de la Torre - A01796701

## ğŸ“Œ Course Information

- Masterâ€™s in Applied Artificial Intelligence
- TecnolÃ³gico de Monterrey
- Instructor: Prof. IvÃ¡n Olmos

---
