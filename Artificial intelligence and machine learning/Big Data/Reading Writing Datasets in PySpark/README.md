# 📂 Big Data with PySpark: Reading and Writing Files

This project focuses on using **PySpark** to read, write, and explore large datasets as part of the Master in Applied Artificial Intelligence program.

## 🚀 Overview

The notebook demonstrates how to:
- Set up a Spark session.
- Read Big Data files using PySpark.
- Write datasets back in different formats.
- Perform initial exploration and manipulation of large datasets.

This exercise introduces core concepts in distributed data processing with PySpark.

## 📊 Key Steps

1. **Library Imports:** Loading the required libraries for PySpark.
2. **Spark Session:** Creating a Spark session to handle distributed computing.
3. **Loading Data:** Reading datasets into Spark DataFrames from files.
4. **Exploration:** Viewing schema, data types, and sample records.
5. **Writing Data:** Exporting processed datasets into various file formats.

## 🛠 Technologies Used

- Python 3.x
- Apache Spark / PySpark
- Jupyter Notebook

## 👨‍🎓 Authors

- Victoria Melgarejo Cabrera - A01795030
- Héctor Alejandro Alvarez Rosas - A01796262
- Andrea Xcaret Gomez Alfaro - A01796384
- Mario Guillen de la Torre - A01796701

## 📌 Course Information

- Master’s in Applied Artificial Intelligence
- Tecnológico de Monterrey
- Instructor: Prof. Iván Olmos

---
